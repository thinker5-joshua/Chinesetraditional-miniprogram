# 使用API上传新增数据到云数据库方案

## 方案概述
将`daily_characters_export.json`中新增的91条汉字数据（charId >= 464）通过云函数API上传到实际的云数据库服务中，使用现有的`import_to_cloud.js`云函数框架，创建专门的新增数据导入脚本。

## 实施步骤

### 1. 准备工作
- 确认云函数环境已配置
- 确认`daily_characters_export.json`文件中包含新增的91条数据
- 确认云数据库集合`daily_characters`已存在

### 2. 数据处理流程
1. **提取新增数据**：
   - 从`daily_characters_export.json`中提取`charId >= 464`的91条数据
   - 验证数据格式和完整性

2. **数据转换**：
   - 将数据转换为云数据库需要的格式
   - 确保数据结构与云数据库一致

3. **创建导入脚本**：
   - 基于现有的`import_to_cloud.js`框架
   - 创建专门的`import_new_data.js`脚本
   - 支持本地测试和云函数调用

4. **导入数据**：
   - 使用云函数API将数据上传到云数据库
   - 支持批量导入和事务处理
   - 处理可能的错误和重试机制

5. **验证结果**：
   - 检查云数据库中新增数据的数量
   - 验证新增数据的完整性
   - 生成导入报告

### 3. 技术实现

#### 3.1 创建数据提取脚本
```javascript
// extract_new_data.js
const fs = require('fs');
const path = require('path');

// 读取云数据库数据
const cloudFilePath = path.join(__dirname, 'daily_characters_export.json');
const cloudData = fs.readFileSync(cloudFilePath, 'utf-8');
const cloudCharacters = JSON.parse(cloudData);

// 提取新增数据（charId >= 464）
const newData = cloudCharacters.filter(char => char.charId >= 464);

// 保存到新文件
const newDataPath = path.join(__dirname, 'new_cloud_data.json');
fs.writeFileSync(newDataPath, JSON.stringify(newData, null, 2), 'utf-8');

console.log(`提取完成，共${newData.length}条新增数据`);
console.log(`保存到文件: ${newDataPath}`);
```

#### 3.2 创建导入脚本
基于现有的`import_to_cloud.js`，创建`import_new_data.js`，修改：
- 读取`new_cloud_data.json`文件
- 调整数据映射（将charId映射为id）
- 优化批量导入逻辑

#### 3.3 核心导入逻辑
```javascript
// 数据映射：将charId转换为id
const mappedData = newData.map(item => ({
  id: item.charId,
  char: item.char,
  correctPronunciation: item.correctPronunciation,
  wrongPronunciations: item.wrongPronunciations,
  relatedPhrases: item.relatedPhrases,
  explanation: item.explanation,
  errorReasonType: item.errorReasonType,
  difficultyLevel: item.difficultyLevel
}));

// 使用现有的importData函数导入
const result = await importData(mappedData);
```

### 4. 执行步骤

1. **提取新增数据**：
   ```bash
   node extract_new_data.js
   ```

2. **本地测试数据**：
   ```bash
   node import_new_data.js
   ```

3. **部署云函数**：
   ```bash
   wx cloud deploy -f import_new_data
   ```

4. **调用云函数**：
   ```javascript
   wx.cloud.callFunction({
     name: 'import_new_data',
     success: res => {
       console.log('导入成功:', res.result);
     },
     fail: err => {
       console.error('导入失败:', err);
     }
   });
   ```

5. **验证导入结果**：
   ```bash
   node verify_import_result.js
   ```

### 5. 验证方案
创建`verify_import_result.js`脚本，用于验证导入结果：
- 检查云数据库中数据总量
- 检查新增数据是否存在
- 验证数据完整性

### 6. 风险与应对
- **导入失败**：实现重试机制，记录失败数据
- **数据重复**：在导入前检查数据是否已存在
- **云函数超时**：减少批次大小，优化导入逻辑
- **权限问题**：确保云函数有足够的数据库操作权限

### 7. 预期结果
- 云数据库中新增91条数据
- 数据总量从462条增加到553条
- 所有新增数据都能正常访问
- 生成详细的导入报告

## 所需创建的文件
1. `extract_new_data.js` - 提取新增数据脚本
2. `import_new_data.js` - 新增数据导入脚本
3. `verify_import_result.js` - 验证导入结果脚本
4. `new_cloud_data.json` - 新增数据临时文件

## 数据映射关系
| 本地字段 | 云数据库字段 | 备注 |
|----------|--------------|------|
| charId   | id           | 主键映射 |
| char     | char         | 保持不变 |
| correctPronunciation | correctPronunciation | 保持不变 |
| wrongPronunciations | wrongPronunciations | 保持不变 |
| relatedPhrases | relatedPhrases | 保持不变 |
| explanation | explanation | 保持不变 |
| errorReasonType | errorReasonType | 保持不变 |
| difficultyLevel | difficultyLevel | 保持不变 |
| createdAt | - | 云数据库自动生成 |
| updatedAt | - | 云数据库自动生成 |

## 执行命令
```bash
# 提取新增数据
node extract_new_data.js

# 本地测试
node import_new_data.js

# 部署云函数
wx cloud deploy -f import_new_data

# 调用云函数（在小程序端或云开发控制台）

# 验证结果
node verify_import_result.js
```

## 预期输出
```
提取完成，共91条新增数据
保存到文件: new_cloud_data.json

本地测试：
读取到91条新增数据
验证数据格式通过
准备导入91条有效数据
开始导入数据，共91条
批次大小: 50
处理批次: 1-50
批次1-50处理完成
处理批次: 51-91
批次51-91处理完成
数据导入完成
成功: 91
失败: 0

导入成功，共91条数据已上传到云数据库
```