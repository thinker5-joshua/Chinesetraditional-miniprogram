### 重新整理诗词数据计划（修订版）

#### 1. 准备工作
- **分析洞天数据**：提取72个地点的名称、ID和相关信息
- **创建别名映射表**：为每个地点创建完整的别名和古地名映射
- **准备脚本工具**：确保现有脚本正常工作，根据需要修改
- **检查数据库连接**：确保chinese-poetry数据库可访问
- **创建数据备份**：备份当前的caves_data.js和poems_data.js

#### 2. 数据提取与整理
- **提取地点列表**：从caves_data.js中提取所有地点信息
- **完善别名映射**：为每个地点添加完整的别名和古地名
- **创建诗词模板**：定义统一的诗词数据格式

#### 3. 批量处理流程
- **分批次处理**：将72个地点分为4个批次，每批次18个地点
- **查找相关诗词**：使用find_location_poems.js脚本为每个地点查找相关诗词
- **验证诗词有效性**：使用comprehensive_validate.js验证诗词真实性和相关性
- **生成待删除列表**：创建需要删除的诗词ID列表，不直接删除
- **手动确认删除**：在删除前生成详细报告，手动确认后再执行删除操作
- **筛选优质诗词**：为每个地点选择3-5首高质量诗词

#### 4. 数据生成与清理
- **批量添加诗词**：使用add_poems.js脚本为每个地点添加诗词
- **重新生成ID**：确保诗词ID连续，无重复
- **清理无效数据**：
  - 生成删除报告，包含待删除诗词的详细信息
  - 手动确认删除列表
  - 执行删除操作
- **格式统一**：确保所有诗词格式一致

#### 5. 验证与报告
- **全面验证**：运行comprehensive_validate.js验证所有诗词
- **生成报告**：创建详细的验证报告，包含：
  - 每个地点的诗词数量和有效性
  - 待删除诗词列表和原因
  - 新增诗词列表和来源
- **手动检查**：抽样检查诗词的相关性和质量
- **确认删除清单**：再次确认删除的诗词，确保没有误删

#### 6. 最终确认与保存
- **确认每个地点的诗词数量**：确保每个地点有3-5首诗词
- **检查数据完整性**：确保所有字段完整，格式正确
- **保存最终数据**：将整理好的数据保存到poems_data.js
- **备份最终数据**：创建数据备份，防止意外丢失

### 执行顺序
1. 准备工作（1-2天）
2. 数据提取与整理（1天）
3. 批量处理第一批次（18个地点，2天）
   - 查找诗词
   - 生成待删除列表
   - 手动确认删除
   - 添加新诗词
4. 批量处理第二批次（18个地点，2天）
5. 批量处理第三批次（18个地点，2天）
6. 批量处理第四批次（18个地点，2天）
7. 验证与报告（1天）
8. 最终确认与保存（1天）

### 技术要点
- 使用现有的脚本工具，根据需要修改
- 优化诗词查找算法，提高效率
- 完善验证逻辑，确保诗词质量
- **添加删除确认机制**：在删除前生成详细报告，手动确认后再执行
- 实现批量处理，减少手动操作
- 生成详细报告，便于跟踪进度

### 预期结果
- 更新后的poems_data.js包含所有72个地点的诗词
- 每个地点有3-5首高质量诗词
- 所有诗词均来自chinese-poetry数据库，真实有效
- 诗词ID连续，格式统一
- 生成完整的验证报告和删除清单

### 风险控制
- **添加删除确认步骤**：防止误删有效诗词
- 遇到数据问题及时调整
- 定期备份数据，防止意外丢失
- 严格按照计划执行，确保进度
- 遇到问题及时沟通解决

### 重点改进
- **删除前确认机制**：在删除任何诗词前，生成详细的删除报告，包含待删除诗词的ID、标题、作者、地点和原因，经过手动确认后再执行删除操作
- **分批次处理**：将72个地点分为4个批次，每批次18个地点，便于管理和监控
- **完善报告体系**：生成详细的处理报告，包含每个步骤的执行情况和结果
- **数据备份机制**：在关键节点创建数据备份，确保数据安全

这个修订版计划更加注重数据安全，添加了删除前的确认步骤，确保不会误删有效诗词。